{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code based on:<br>\n",
    "https://shravan-kuchkula.github.io/scrape_clean_normalize_gutenberg_text/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#  Module: scrapeGutenberg.py\n",
    "#  Author: Shravan Kuchkula\n",
    "#  Date: 05/24/2019\n",
    "##############################\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def getTitlesAndAuthors(title_and_authors):\n",
    "    titles = []\n",
    "    authors = []\n",
    "    for ta in title_and_authors:\n",
    "        titles.append(ta[0])\n",
    "        authors.append(ta[1])\n",
    "    return titles, authors\n",
    "\n",
    "def getBookURLsFromBookShelf(bookshelf):\n",
    "    \n",
    "    # make a request and get a response object\n",
    "    response = requests.get(bookshelf)\n",
    "    \n",
    "    # get the source from the response object\n",
    "    source = response.text\n",
    "    \n",
    "    # construct the soup object\n",
    "    soup = BeautifulSoup(source, 'html.parser')\n",
    "    \n",
    "    # get all the a tags\n",
    "    tags = soup.find_all('a', attrs={'class': 'extiw'})\n",
    "    \n",
    "    # get all the urls\n",
    "    urls = [\"http:\" + tag.attrs['href'] for tag in tags]\n",
    "    \n",
    "    # construct the soup\n",
    "    soups = [BeautifulSoup(requests.get(url).text, 'html.parser') for url in urls]\n",
    "    \n",
    "    # get all the plain text files\n",
    "    href_tags = [soup.find(href=True, text='Plain Text UTF-8') for soup in soups]\n",
    "\n",
    "    # get all the book urls\n",
    "    book_urls = [\"http:\" + tag.attrs['href'] for tag in href_tags]\n",
    "    \n",
    "    # get h1 tags for getting titles and authors\n",
    "    h1_tags = [soup.find('h1').getText() for soup in soups]\n",
    "    \n",
    "    # construct titles and authors list\n",
    "    title_and_authors = [re.split(r'by', tag) for tag in h1_tags]\n",
    "\n",
    "    # some titles don't have authors, so add Unknown to author\n",
    "    for ta in title_and_authors:\n",
    "        if len(ta) == 1:\n",
    "            ta.append(\"Unknown\")\n",
    "    \n",
    "    # get the titles and authors into their own lists\n",
    "    titles, authors = getTitlesAndAuthors(title_and_authors)\n",
    "    \n",
    "    return book_urls, titles, authors, soup\n",
    "\n",
    "def getCategories(soup, books):\n",
    "    # get all the tags\n",
    "    tags = soup.find_all('a', attrs={'class': 'extiw'})\n",
    "\n",
    "    # get all the titles\n",
    "    title_id = [tag.attrs['title'] for tag in tags]\n",
    "\n",
    "    # clean the title\n",
    "    title_ids = [title.split(':')[1] for title in title_id]\n",
    "\n",
    "    # create a new column\n",
    "    books['title_id'] = title_ids\n",
    "\n",
    "    # create a categories column\n",
    "    books['category'] = \"\"\n",
    "\n",
    "    # get the categories from h3 tags\n",
    "    for h3 in soup.find_all('h3'):\n",
    "        #print(h3.getText())\n",
    "        category = h3.getText()\n",
    "        h3_atags = h3.findNextSibling().find_all('a', attrs={'class': 'extiw'})\n",
    "        for tag in h3_atags:\n",
    "            #print(tag['title'].split(':')[1])\n",
    "            book_id = tag['title'].split(':')[1]\n",
    "            books['category'].iloc[np.where(books.title_id == book_id)] = category\n",
    "\n",
    "    # get the categories from h2 tags\n",
    "    for tag in soup.find_all('h2'):\n",
    "        if len(tag.findChildren()) > 0:\n",
    "            for t in tag.children:\n",
    "                if t.getText() != 'Readers' and t.getText() != 'Uncategorized':\n",
    "                    #print(t.getText())\n",
    "                    category = t.getText()\n",
    "                    h2_atags = tag.findNextSibling().find_all('a', attrs={'class': 'extiw'})\n",
    "                    for atag in h2_atags:\n",
    "                        book_id = atag['title'].split(':')[1]\n",
    "                        books['category'].iloc[np.where(books.title_id == book_id)] = category\n",
    "\n",
    "    # remaining links are uncategorized\n",
    "    books['category'].iloc[np.where(books.category == '')] = 'Uncategorized'\n",
    "    \n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#  Module: gutenbergPreprocessing.py\n",
    "#  Author: Shravan Kuchkula\n",
    "#  Date: 05/24/2019\n",
    "########################################\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_gutenburg_headers(book_text):\n",
    "    book_text = book_text.replace('\\r', '')\n",
    "    book_text = book_text.replace('\\n', ' ')\n",
    "    start_match = re.search(r'\\*{3}\\s?START.+?\\*{3}', book_text)\n",
    "    end_match = re.search(r'\\*{3}\\s?END.+?\\*{3}', book_text)\n",
    "    try:\n",
    "        book_text = book_text[start_match.span()[1]:end_match.span()[0]]\n",
    "    except AttributeError:\n",
    "        print('No match found')    \n",
    "    return book_text\n",
    "\n",
    "def remove_gutenberg_footer(book_text):\n",
    "    if book_text.find('End of the Project Gutenberg') != -1:\n",
    "        book_text = book_text[:book_text.find('End of the Project Gutenberg')]\n",
    "    elif book_text.find('End of Project Gutenberg') != -1:\n",
    "        book_text = book_text[:book_text.find('End of Project Gutenberg')]\n",
    "    return book_text\n",
    "\n",
    "def getTextFromURLByRemovingHeaders(book_urls):\n",
    "    book_texts = []\n",
    "    for url in book_urls:\n",
    "        book_text = requests.get(url).text\n",
    "        book_text = remove_gutenburg_headers(book_text)\n",
    "        book_texts.append(remove_gutenberg_footer(book_text))\n",
    "    return book_texts\n",
    "\n",
    "def searchPossibleStarts(pattern, book):\n",
    "    match = re.search(pattern, book, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.span()[0]\n",
    "    return -1\n",
    "\n",
    "def moveToStartOfTheBook(possible_starts, book):\n",
    "    # construct start indexes\n",
    "    start_indexes = [searchPossibleStarts(ps, book) for ps in possible_starts]\n",
    "    \n",
    "    # calculate the lowest index of the list of possible values. Use that as the start index.\n",
    "    # TODO: this throws an exception when nothing is found\n",
    "    min_index = min(list(filter(lambda x: x != -1, start_indexes)))\n",
    "    \n",
    "    if min_index > -1:\n",
    "        return book[min_index:]\n",
    "    else:\n",
    "        print(\"Match not found in possible_starts, update your possible_starts\")\n",
    "    \n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#  Module: gutenbergTextNormalization.py\n",
    "#  Author: Shravan Kuchkula\n",
    "#  Date: 05/24/2019\n",
    "########################################\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# tokenize text\n",
    "def tokenize_text(book_text):\n",
    "    TOKEN_PATTERN = r'\\s+'\n",
    "    regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=True)\n",
    "    word_tokens = regex_wt.tokenize(book_text)\n",
    "    return word_tokens\n",
    "\n",
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation))) \n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens]) \n",
    "    return filtered_tokens\n",
    "\n",
    "def convert_to_lowercase(tokens):\n",
    "    return [token.lower() for token in tokens if token.isalpha()]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    return filtered_tokens\n",
    "\n",
    "def apply_stemming_and_lemmatize(tokens, ls=LancasterStemmer(), wnl=WordNetLemmatizer()):\n",
    "    return [wnl.lemmatize(ls.stem(token)) for token in tokens]\n",
    "\n",
    "def cleanTextBooks(book_texts):\n",
    "    clean_books = []\n",
    "    for book in book_texts:\n",
    "        book_i = tokenize_text(book)\n",
    "        book_i = remove_characters_after_tokenization(book_i)\n",
    "        book_i = convert_to_lowercase(book_i)\n",
    "        book_i = remove_stopwords(book_i)\n",
    "        book_i = apply_stemming_and_lemmatize(book_i)\n",
    "        clean_books.append(book_i)\n",
    "    return clean_books\n",
    "\n",
    "def normalizedVocabularyScore(clean_books):\n",
    "    v_size = [len(set(book)) for book in clean_books]\n",
    "    max_v_size = np.max(v_size)\n",
    "    v_raw_score = v_size/max_v_size\n",
    "    v_sqrt_score = np.sqrt(v_raw_score)\n",
    "    v_rank_score = pd.Series(v_size).rank()/len(v_size)\n",
    "    v_final_score = (pd.Series(v_sqrt_score) + v_rank_score)/2\n",
    "    \n",
    "    return pd.DataFrame({'v_size': v_size,\n",
    "                        'v_raw_score': v_raw_score,\n",
    "                        'v_sqrt_score': v_sqrt_score,\n",
    "                        'v_rank_score': v_rank_score,\n",
    "                        'v_final_score': v_final_score})\n",
    "\n",
    "def longWordVocabularySize(clean_book, minChar=10):\n",
    "    V = set(clean_book)\n",
    "    long_words = [w for w in V if len(w) > minChar]\n",
    "    return len(long_words)\n",
    "\n",
    "def normalizedLongWordVocabularyScore(clean_books):\n",
    "    lw_v_size = [longWordVocabularySize(book) for book in clean_books]\n",
    "    max_v_size = np.max(lw_v_size)\n",
    "    v_raw_score = lw_v_size/max_v_size\n",
    "    v_sqrt_score = np.sqrt(v_raw_score)\n",
    "    v_rank_score = pd.Series(lw_v_size).rank()/len(lw_v_size)\n",
    "    lw_v_final_score = (pd.Series(v_sqrt_score) + v_rank_score)/2\n",
    "    \n",
    "    return pd.DataFrame({'lw_v_size': lw_v_size,\n",
    "                        'lw_v_final_score': lw_v_final_score})\n",
    "\n",
    "\n",
    "def textDifficultyScore(clean_books):\n",
    "    df_vocab_scores = normalizedVocabularyScore(clean_books)\n",
    "    df_lw_vocab_scores = normalizedLongWordVocabularyScore(clean_books)\n",
    "    lexical_diversity_scores = [len(set(book))/len(book) for book in clean_books]\n",
    "    \n",
    "    text_difficulty = (df_vocab_scores['v_final_score'] + \\\n",
    "                     df_lw_vocab_scores['lw_v_final_score'] + \\\n",
    "                     lexical_diversity_scores)/3\n",
    "    \n",
    "    return pd.DataFrame({'text_difficulty': text_difficulty})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from scrapeGutenberg import *\n",
    "#from gutenbergPreprocessing import *\n",
    "#from gutenbergTextNormalization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clayj\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>author(s)</th>\n",
       "      <th>title_id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [url, title, author(s), title_id, category]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bookshelf url\n",
    "bookshelf = 'http://www.gutenberg.org/wiki/Children%27s_Instructional_Books_(Bookshelf)'\n",
    "\n",
    "# from the bookshelf get all the book_urls, titles, \n",
    "# authors and soup(for getting categories)\n",
    "book_urls, titles, authors, soup = getBookURLsFromBookShelf(bookshelf)\n",
    "\n",
    "# construct a books dataframe\n",
    "books = pd.DataFrame({'url': book_urls, 'title': titles, 'author(s)': authors})\n",
    "\n",
    "# get books df with categories\n",
    "books = getCategories(soup, books)\n",
    "\n",
    "# with categories\n",
    "display(books.shape)\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
